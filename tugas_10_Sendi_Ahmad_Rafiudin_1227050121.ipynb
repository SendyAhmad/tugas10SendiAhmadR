{
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "nicapotato_womens_ecommerce_clothing_reviews_path = kagglehub.dataset_download('nicapotato/womens-ecommerce-clothing-reviews')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "zmGHHOfGrZZ8"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "c9750b268761fcc864b826c4198689026cf2d1a6",
        "id": "U0UzQu6CrZZ_"
      },
      "cell_type": "markdown",
      "source": [
        "<hr/>\n",
        "# **Predicting Sentiment from Clothing Reviews**\n",
        "\n",
        "[**Burhan Y. Kiyakoglu**](https://www.kaggle.com/burhanykiyakoglu)\n",
        "<hr/>\n",
        "<span id=\"0\"></span>\n",
        "<font color=green>\n",
        "1. [Overview](#1)\n",
        "1. [Importing Modules and Reading the Dataset](#2)\n",
        "1. [Adding the Word Counts to the Dataframe and Finding out How Many Times Some Words Were Used ](#3)\n",
        "1. [Demonstrating the Densities of Class Names, Some Selected Words and All Words in the Reviews By Using WordCloud](#4)\n",
        "1. [Viewing the Relation Between Ratings, Class Names and Age](#5)\n",
        "1. [Building a Sentiment Classifier](#6)\n",
        "   * [Logistic Regression ](#7)\n",
        "   * [Naive Bayes](#8)\n",
        "   * [Support Vector Machine (SVM)](#9)\n",
        "   * [Neural Network](#10)\n",
        "1. [Evaluating Models](#11)\n",
        "   * [Adding Results to the Dataframe](#12)\n",
        "   * [ROC Curves and AUC](#13)\n",
        "   * [Confusion Matrices](#14)\n",
        "   * [Precision - Recall - F1-Score](#15)\n",
        "1. [Conclusion](#16)"
      ]
    },
    {
      "metadata": {
        "_uuid": "1d07c6dedfbe45292ad7614b414dbf29c14a9070",
        "id": "NmujXt2trZaC"
      },
      "cell_type": "markdown",
      "source": [
        "# <span id=\"1\"></span> Overview\n",
        "<hr/>\n",
        "Welcome to my Kernel! In this kernel, I use various methods and try to predict the customer sentiment from reviews. As you can guess, there are various methods to suceed this and each method has pros and cons. I think **Naive Bayes is one of the most important methods in text mining because it is the fastest**. However, if we do not have a very large dataset or have high CPU power, the others might be useful too.\n",
        "\n",
        "Here, I started with examining the data and use some charts to reach more insights. Afterwards, I applied different models and compared them from various aspects.\n",
        "\n",
        "If you have a question or feedback, do not hesitate to write and if you like this kernel, please <b><font color=\"red\">do not forget to </font><font color=\"green\">UPVOTE </font></b> ðŸ™‚\n",
        "<br/>\n",
        "<img src=\"https://i.imgur.com/1tHbhZ4.jpg\" title=\"source: imgur.com\"/>"
      ]
    },
    {
      "metadata": {
        "_uuid": "5636cf459775f4c41e79a2117e7049d74f39488d",
        "id": "-WQ5k0zdrZaG"
      },
      "cell_type": "markdown",
      "source": [
        "# <span id=\"2\"></span> Importing Modules and Reading the Dataset\n",
        "#### [Return Contents](#0)\n",
        "<hr/>"
      ]
    },
    {
      "metadata": {
        "_uuid": "c1436e92c2e44d5142e0947321d7c4b833245ad5",
        "id": "MCIO6gSyrZaH"
      },
      "cell_type": "markdown",
      "source": [
        "In order to make some analysis, we need to set our environment up. To do this, I firstly imported some modules and read the data. The below output is the head of the data but if you want to see more details, you might try removing ***#*** signs in front of the ***df.describe()*** and ***df.info()***.  \n",
        "\n",
        "Further, I decided that I do not need some columns and defined a new dataset which just has the columns I used."
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "3crWblr8rZaJ"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import sklearn.metrics as mt\n",
        "from plotly import tools\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "df1 = pd.read_csv('../input/Womens Clothing E-Commerce Reviews.csv')\n",
        "df = df1[['Review Text','Rating','Class Name','Age']]\n",
        "#df1.info()\n",
        "#df1.describe()\n",
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b3e18f15da8f15842316c82e248a8d8752e5fcba",
        "id": "ZgZbLRozrZaK"
      },
      "cell_type": "markdown",
      "source": [
        "# <span id=\"3\"></span> Adding the Word Counts to the Dataframe and Finding out How Many Times Some Words Were Used\n",
        "#### [Return Contents](#0)\n",
        "<hr/>"
      ]
    },
    {
      "metadata": {
        "_uuid": "9f033a6c9940ef4e93c1e621db00e50a70627817",
        "id": "agmshot-rZaM"
      },
      "cell_type": "markdown",
      "source": [
        "Adding the word counts to a dataframe is a very good practice because we might use these counts to reach some useful information. To do this, I defined the function ***wordcounts***."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1cda9ceace361f152f30fa199f021528d0f9f2b7",
        "_kg_hide-output": false,
        "_kg_hide-input": true,
        "id": "IN11a2VUrZaN"
      },
      "cell_type": "code",
      "source": [
        "# fill NA values by space\n",
        "df['Review Text'] = df['Review Text'].fillna('')\n",
        "\n",
        "# CountVectorizer() converts a collection\n",
        "# of text documents to a matrix of token counts\n",
        "vectorizer = CountVectorizer()\n",
        "# assign a shorter name for the analyze\n",
        "# which tokenizes the string\n",
        "analyzer = vectorizer.build_analyzer()\n",
        "\n",
        "def wordcounts(s):\n",
        "    c = {}\n",
        "    # tokenize the string and continue, if it is not empty\n",
        "    if analyzer(s):\n",
        "        d = {}\n",
        "        # find counts of the vocabularies and transform to array\n",
        "        w = vectorizer.fit_transform([s]).toarray()\n",
        "        # vocabulary and index (index of w)\n",
        "        vc = vectorizer.vocabulary_\n",
        "        # items() transforms the dictionary's (word, index) tuple pairs\n",
        "        for k,v in vc.items():\n",
        "            d[v]=k # d -> index:word\n",
        "        for index,i in enumerate(w[0]):\n",
        "            c[d[index]] = i # c -> word:count\n",
        "    return  c\n",
        "\n",
        "# add new column to the dataframe\n",
        "df['Word Counts'] = df['Review Text'].apply(wordcounts)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "984a74c85fce1587d0116d6a75610d57a185421c",
        "id": "DrlHejlYrZaP"
      },
      "cell_type": "markdown",
      "source": [
        "# <span id=\"4\"></span>  Demonstrating the Densities of Class Names, Some Selected Words and All Words in the Reviews By Using WordCloud\n",
        "#### [Return Contents](#0)\n",
        "<hr/>"
      ]
    },
    {
      "metadata": {
        "_uuid": "d693c4ce5c6d86a60387de5e034f4991db3d2386",
        "id": "_YhjO-tWrZaQ"
      },
      "cell_type": "markdown",
      "source": [
        "In this section, I demonstrated the word densities which can be very informative. First, I selected some words which show the customer sentiments like love, hate, fantastic or regret. Second, since we do not know the product names, I decided to check the product class names. By doing this, we may at least learn the most prefered classes. Further, I thought that looking at the densities of all words in the reviews might be interesting. Lastly, I used the *WordCloud* module and printed the first five lines of the tables which shows the word counts for the selected words and the class names.\n",
        "\n",
        "It can be observed from the below figures and tables that positive words as love, great, super were used more. When we look at the classes, customers mostly prefered dress, knits and blouses. We may also see that dress and love are in the freequently used words within all reviews."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ddd45a2082a6a6a70642ec16fca3f4b355da1285",
        "_kg_hide-input": true,
        "id": "LYvbwag4rZaR"
      },
      "cell_type": "code",
      "source": [
        "# selecting some words to examine detailed\n",
        "selectedwords = ['awesome','great','fantastic','extraordinary','amazing','super',\n",
        "                 'magnificent','stunning','impressive','wonderful','breathtaking',\n",
        "                 'love','content','pleased','happy','glad','satisfied','lucky',\n",
        "                 'shocking','cheerful','wow','sad','unhappy','horrible','regret',\n",
        "                 'bad','terrible','annoyed','disappointed','upset','awful','hate']\n",
        "\n",
        "def selectedcount(dic,word):\n",
        "    if word in dic:\n",
        "        return dic[word]\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "dfwc = df.copy()\n",
        "for word in selectedwords:\n",
        "    dfwc[word] = dfwc['Word Counts'].apply(selectedcount,args=(word,))\n",
        "\n",
        "word_sum = dfwc[selectedwords].sum()\n",
        "print('Selected Words')\n",
        "print(word_sum.sort_values(ascending=False).iloc[:5])\n",
        "\n",
        "print('\\nClass Names')\n",
        "print(df['Class Name'].fillna(\"Empty\").value_counts().iloc[:5])\n",
        "\n",
        "fig, ax = plt.subplots(1,2,figsize=(20,10))\n",
        "wc0 = WordCloud(background_color='white',\n",
        "                      width=450,\n",
        "                      height=400 ).generate_from_frequencies(word_sum)\n",
        "\n",
        "cn = df['Class Name'].fillna(\" \").value_counts()\n",
        "wc1 = WordCloud(background_color='white',\n",
        "                      width=450,\n",
        "                      height=400\n",
        "                     ).generate_from_frequencies(cn)\n",
        "\n",
        "ax[0].imshow(wc0)\n",
        "ax[0].set_title('Selected Words\\n',size=25)\n",
        "ax[0].axis('off')\n",
        "\n",
        "ax[1].imshow(wc1)\n",
        "ax[1].set_title('Class Names\\n',size=25)\n",
        "ax[1].axis('off')\n",
        "\n",
        "rt = df['Review Text']\n",
        "plt.subplots(figsize=(18,6))\n",
        "wordcloud = WordCloud(background_color='white',\n",
        "                      width=900,\n",
        "                      height=300\n",
        "                     ).generate(\" \".join(rt))\n",
        "plt.imshow(wordcloud)\n",
        "plt.title('All Words in the Reviews\\n',size=25)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a3f7f0b362d9171cce9eef2f32f1c6ca7d4a74f2",
        "id": "HWuAN1e3rZaS"
      },
      "cell_type": "markdown",
      "source": [
        " # <span id=\"5\"></span> Viewing the Relation Between Rating, Class Name and Age  \n",
        "#### [Return Contents](#0)\n",
        "<hr/>"
      ]
    },
    {
      "metadata": {
        "_uuid": "07e834b7754e0b6c78cc557b3bc693bdac5cf775",
        "id": "b9Qp9OdwrZaT"
      },
      "cell_type": "markdown",
      "source": [
        "I thought that examining the relation between the rating, class names and age might be good because some age groups may always give low ratings or make negative reviews or prefer the products in the same class. To examine this relationship, I used the below dynamic charts."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6f4a2b818be0f6364e6bec4cebf94ba13a869432",
        "_kg_hide-input": true,
        "id": "gDS-1ugqrZaU"
      },
      "cell_type": "code",
      "source": [
        "df1=df['Rating'].value_counts().to_frame()\n",
        "avgdf1 = df.groupby('Class Name').agg({'Rating': np.average})\n",
        "avgdf2 = df.groupby('Class Name').agg({'Age': np.average})\n",
        "avgdf3 = df.groupby('Rating').agg({'Age': np.average})\n",
        "\n",
        "trace1 = go.Bar(\n",
        "    x=avgdf1.index,\n",
        "    y=round(avgdf1['Rating'],2),\n",
        "    marker=dict(\n",
        "        color=avgdf1['Rating'],\n",
        "        colorscale = 'RdBu')\n",
        ")\n",
        "\n",
        "trace2 = go.Bar(\n",
        "    x=df1.index,\n",
        "    y=df1.Rating,\n",
        "    marker=dict(\n",
        "        color=df1['Rating'],\n",
        "        colorscale = 'RdBu')\n",
        ")\n",
        "\n",
        "trace3 = go.Bar(\n",
        "    x=avgdf2.index,\n",
        "    y=round(avgdf2['Age'],2),\n",
        "    marker=dict(\n",
        "        color=avgdf2['Age'],\n",
        "        colorscale = 'RdBu')\n",
        ")\n",
        "\n",
        "trace4 = go.Bar(\n",
        "    x=avgdf3.index,\n",
        "    y=round(avgdf3['Age'],2),\n",
        "    marker=dict(\n",
        "        color=avgdf3['Age'],\n",
        "        colorscale = 'Reds')\n",
        ")\n",
        "\n",
        "fig = tools.make_subplots(rows=2, cols=2, print_grid=False)\n",
        "\n",
        "fig.append_trace(trace1, 1, 1)\n",
        "fig.append_trace(trace2, 1, 2)\n",
        "fig.append_trace(trace3, 2, 1)\n",
        "fig.append_trace(trace4, 2, 2)\n",
        "\n",
        "fig['layout']['xaxis1'].update(title='Class')\n",
        "fig['layout']['yaxis1'].update(title='Average Rating')\n",
        "fig['layout']['xaxis2'].update(title='Rating')\n",
        "fig['layout']['yaxis2'].update(title='Count')\n",
        "fig['layout']['xaxis3'].update(title='Class')\n",
        "fig['layout']['yaxis3'].update(title='Average Age of the Reviewers')\n",
        "fig['layout']['xaxis4'].update(title='Rating')\n",
        "fig['layout']['yaxis4'].update(title='Average Age of the Reviewers')\n",
        "\n",
        "fig['layout'].update(height=800, width=900,showlegend=False)\n",
        "fig.update_layout({'plot_bgcolor':'rgba(0,0,0,0)',\n",
        "                   'paper_bgcolor':'rgba(0,0,0,0)'})\n",
        "#fig['layout'].update(plot_bgcolor='rgba(0,0,0,0)')\n",
        "#fig['layout'].update(paper_bgcolor='rgba(0,0,0,0)')\n",
        "py.iplot(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c688babf6822d1b322ae38d34264d2adce7b5e97",
        "id": "dDTfQg-5rZaY"
      },
      "cell_type": "markdown",
      "source": [
        "It seems that most of the ratings are positive and the average rating between the classes looks close. On the other hand, when we look at ages, average age does not change significantly according to the rating. Also, average age changes slightly between class names except casual bottoms. We can disregard casual bottoms because the below chart shows that there are just two reviews and making an inference will not be right."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fbf76a6f3e33e712706170704857fd44634571fb",
        "_kg_hide-input": true,
        "id": "uYN65JmhrZaZ"
      },
      "cell_type": "code",
      "source": [
        "cv = df['Class Name'].value_counts()\n",
        "\n",
        "trace = go.Scatter3d( x = avgdf1.index,\n",
        "                      y = avgdf1['Rating'],\n",
        "                      z = cv[avgdf1.index],\n",
        "                      mode = 'markers',\n",
        "                      marker = dict(size=10,color=avgdf1['Rating']),\n",
        "                      hoverinfo =\"text\",\n",
        "                      text=\"Class: \"+avgdf1.index+\" \\ Average Rating: \"+avgdf1['Rating'].map(' {:,.2f}'.format).apply(str)+\" \\ Number of Reviewers: \"+cv[avgdf1.index].apply(str)\n",
        "                      )\n",
        "\n",
        "data = [trace]\n",
        "layout = go.Layout(title=\"Average Rating & Class & Number of Reviewers\",\n",
        "                   scene = dict(\n",
        "                    xaxis = dict(title='Class'),\n",
        "                    yaxis = dict(title='Average Rating'),\n",
        "                    zaxis = dict(title='Number of Sales'),),\n",
        "                   margin = dict(l=30, r=30, b=30, t=30))\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "py.iplot(fig)\n",
        "plt.savefig('3D_Scatter.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "96797ed17ba3cd3c6aa41bdef38cb10deb409341",
        "id": "1w4v9DFxrZaa"
      },
      "cell_type": "markdown",
      "source": [
        " # <span id=\"6\"></span> Building a Sentiment Classifier\n",
        "#### [Return Contents](#0)\n",
        "<hr/>"
      ]
    },
    {
      "metadata": {
        "_uuid": "a0ca41ad24d601e6eed1dd42dc592ac927edc7a7",
        "id": "uIYXlZwRrZab"
      },
      "cell_type": "markdown",
      "source": [
        "Since we do not have a column which shows the sentiment as positive or negative in the dataset, I defined a new sentiment column. To do this, I assumed the reviews which has **4 or higher ** rating as **positive (True in the new dataframe)** and **2 or lower** rating as **negative (False in the new dataframe)**. Also, I did not include the lines that has **neutral** ratings which are equal to **3**. Following that, I splitted the data as training and test sets."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ef4e5b9dac84bbbb9bf3a08b5e63e75a70514dfe",
        "_kg_hide-input": true,
        "id": "k7WrrRoGrZab"
      },
      "cell_type": "code",
      "source": [
        "# Rating of 4 or higher -> positive, while the ones with\n",
        "# Rating of 2 or lower -> negative\n",
        "# Rating of 3 -> neutral\n",
        "df = df[df['Rating'] != 3]\n",
        "df['Sentiment'] = df['Rating'] >=4\n",
        "df.head()\n",
        "\n",
        "# split data\n",
        "train_data,test_data = train_test_split(df,train_size=0.8,random_state=0)\n",
        "# select the columns and\n",
        "# prepare data for the models\n",
        "X_train = vectorizer.fit_transform(train_data['Review Text'])\n",
        "y_train = train_data['Sentiment']\n",
        "X_test = vectorizer.transform(test_data['Review Text'])\n",
        "y_test = test_data['Sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6123ca5c04391db3023f31c3363474db1c8afee5",
        "id": "gqKf2kgtrZac"
      },
      "cell_type": "markdown",
      "source": [
        "Then, I fitted the models one by one. Since, some of them take too much time, running each of them in different cells is a better choice.   "
      ]
    },
    {
      "metadata": {
        "_uuid": "dd935dbe4f4fdbd0a721695bffb72c2778b16690",
        "id": "fmArOosDrZac"
      },
      "cell_type": "markdown",
      "source": [
        "## <span id=\"7\"></span> Logistic Regression"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b78e37c89e02fd1dea22b87509dc871f5ad973c6",
        "id": "9kcp7Wo6rZad"
      },
      "cell_type": "code",
      "source": [
        "start=dt.datetime.now()\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train,y_train)\n",
        "print('Elapsed time: ',str(dt.datetime.now()-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ad4bd4a855ddc670eedf280b3a81efa9a57d470e",
        "id": "nslW-cN3rZae"
      },
      "cell_type": "markdown",
      "source": [
        "## <span id=\"8\"></span> Naive Bayes"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4715c54fec338f8017b2f5597490c9a16ce4f101",
        "id": "282QsDTVrZae"
      },
      "cell_type": "code",
      "source": [
        "start=dt.datetime.now()\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train,y_train)\n",
        "print('Elapsed time: ',str(dt.datetime.now()-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "daaf5d10642b2ba0e89846091f7a6b4a1de4252b",
        "id": "7rYj5CptrZae"
      },
      "cell_type": "markdown",
      "source": [
        "## <span id=\"9\"></span> Support Vector Machine (SVM)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab7e1fcbe45995e034124b97ae5d2674554abd66",
        "id": "9BdsJi-mrZaf"
      },
      "cell_type": "code",
      "source": [
        "start=dt.datetime.now()\n",
        "svm = SVC()\n",
        "svm.fit(X_train,y_train)\n",
        "print('Elapsed time: ',str(dt.datetime.now()-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "048d88c13016a4f8a39d1427c3eeaf01d8a4f2b9",
        "id": "vY-j0reYrZaf"
      },
      "cell_type": "markdown",
      "source": [
        "## <span id=\"10\"></span> Neural Network"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3dfb24752703bcadb607fe5bd43026941b41a65a",
        "id": "RE-WlrPbrZaf"
      },
      "cell_type": "code",
      "source": [
        "start=dt.datetime.now()\n",
        "nn = MLPClassifier()\n",
        "nn.fit(X_train,y_train)\n",
        "print('Elapsed time: ',str(dt.datetime.now()-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e37a1b005158073c90e0813313a15428aefb0f02",
        "id": "Prf3baYZrZag"
      },
      "cell_type": "markdown",
      "source": [
        "# <span id=\"11\"></span> Evaluating Models\n",
        "#### [Return Contents](#0)\n",
        "<hr/>"
      ]
    },
    {
      "metadata": {
        "_uuid": "c5e0df1e9c4fc2e9b6c00c57d5b8be355be89b7e",
        "id": "0mtL6P6vrZag"
      },
      "cell_type": "markdown",
      "source": [
        "## <span id=\"12\"></span> Adding Results to the Dataframe"
      ]
    },
    {
      "metadata": {
        "_uuid": "7550e005914243acf6e7928fe76e3e67cae3ae64",
        "id": "9pQPu33HrZag"
      },
      "cell_type": "markdown",
      "source": [
        "At first, I added the prediction results to my training data. However, if you want to observe the prediction probabilies, you might use the commented out code."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "268d92918c01136e0222364060abf37d91422849",
        "_kg_hide-input": true,
        "id": "prNDmIp8rZah"
      },
      "cell_type": "code",
      "source": [
        "# define a dataframe for the prediction probablities of the models\n",
        "#df1 = train_data.copy()\n",
        "#df1['Logistic Regression'] = lr.predict_proba(X_train)[:,1]\n",
        "#df1['Naive Bayes'] = nb.predict_proba(X_train)[:,1]\n",
        "#df1['SVM'] = svm.decision_function(X_train)\n",
        "#df1['Neural Network'] = nn.predict_proba(X_train)[:,1]\n",
        "#df1=df1.round(2)\n",
        "#df1.head()\n",
        "\n",
        "# define a dataframe for the predictions\n",
        "df2 = train_data.copy()\n",
        "df2['Logistic Regression'] = lr.predict(X_train)\n",
        "df2['Naive Bayes'] = nb.predict(X_train)\n",
        "df2['SVM'] = svm.predict(X_train)\n",
        "df2['Neural Network'] = nn.predict(X_train)\n",
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0ca48e96a1af34602deeed24ee1df8c09b5f0012",
        "id": "MBz_VwlarZah"
      },
      "cell_type": "markdown",
      "source": [
        "## <span id=\"13\"></span> ROC Curves and AUC"
      ]
    },
    {
      "metadata": {
        "id": "Yhr7RKnprZai"
      },
      "cell_type": "markdown",
      "source": [
        "I started my evaluation with ROC curve and AUC. As you may observe below, results look pretty good but it does not give much insight. To decide which model is the best we must also examine other evaluation metrics."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "967355c906e983f36af66b7cc3b6fb43f2447e7c",
        "_kg_hide-input": true,
        "id": "7l6LWsJ1rZai"
      },
      "cell_type": "code",
      "source": [
        "pred_lr = lr.predict_proba(X_test)[:,1]\n",
        "fpr_lr,tpr_lr,_ = roc_curve(y_test,pred_lr)\n",
        "roc_auc_lr = auc(fpr_lr,tpr_lr)\n",
        "\n",
        "pred_nb = nb.predict_proba(X_test)[:,1]\n",
        "fpr_nb,tpr_nb,_ = roc_curve(y_test.values,pred_nb)\n",
        "roc_auc_nb = auc(fpr_nb,tpr_nb)\n",
        "\n",
        "pred_svm = svm.decision_function(X_test)\n",
        "fpr_svm,tpr_svm,_ = roc_curve(y_test.values,pred_svm)\n",
        "roc_auc_svm = auc(fpr_svm,tpr_svm)\n",
        "\n",
        "pred_nn = nn.predict_proba(X_test)[:,1]\n",
        "fpr_nn,tpr_nn,_ = roc_curve(y_test.values,pred_nn)\n",
        "roc_auc_nn = auc(fpr_nn,tpr_nn)\n",
        "\n",
        "f, axes = plt.subplots(2, 2,figsize=(15,10))\n",
        "axes[0,0].plot(fpr_lr, tpr_lr, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_lr))\n",
        "axes[0,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "axes[0,0].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\n",
        "axes[0,0].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Logistic Regression')\n",
        "axes[0,0].legend(loc='lower right', fontsize=13)\n",
        "\n",
        "axes[0,1].plot(fpr_nb, tpr_nb, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_nb))\n",
        "axes[0,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "axes[0,1].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\n",
        "axes[0,1].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Naive Bayes')\n",
        "axes[0,1].legend(loc='lower right', fontsize=13)\n",
        "\n",
        "axes[1,0].plot(fpr_svm, tpr_svm, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_svm))\n",
        "axes[1,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "axes[1,0].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\n",
        "axes[1,0].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Support Vector Machine')\n",
        "axes[1,0].legend(loc='lower right', fontsize=13)\n",
        "\n",
        "axes[1,1].plot(fpr_nn, tpr_nn, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_nn))\n",
        "axes[1,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "axes[1,1].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\n",
        "axes[1,1].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Neural Network')\n",
        "axes[1,1].legend(loc='lower right', fontsize=13);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5ea828d90388d25949f285634db0546ef329d869",
        "id": "mUCjdIU7rZai"
      },
      "cell_type": "markdown",
      "source": [
        "## <span id=\"14\"></span> Confusion Matrices"
      ]
    },
    {
      "metadata": {
        "id": "iZcgYUQcrZaj"
      },
      "cell_type": "markdown",
      "source": [
        "To reach more information, I also used confusion matrices. It can be seen that SVM does not give healthy results although it has high ROC values."
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "b8eb4577a0c1a15ac00b554eebbcbbbd2dbf3ee8",
        "id": "ENoLYazSrZaj"
      },
      "cell_type": "code",
      "source": [
        "# preparation for the confusion matrix\n",
        "lr_cm=confusion_matrix(y_test.values, lr.predict(X_test))\n",
        "nb_cm=confusion_matrix(y_test.values, nb.predict(X_test))\n",
        "svm_cm=confusion_matrix(y_test.values, svm.predict(X_test))\n",
        "nn_cm=confusion_matrix(y_test.values, nn.predict(X_test))\n",
        "\n",
        "plt.figure(figsize=(15,12))\n",
        "plt.suptitle(\"Confusion Matrices\",fontsize=24)\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "plt.title(\"Logistic Regression\")\n",
        "sns.heatmap(lr_cm, annot = True, cmap=\"Greens\",cbar=False);\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.title(\"Naive Bayes\")\n",
        "sns.heatmap(nb_cm, annot = True, cmap=\"Greens\",cbar=False);\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "plt.title(\"Support Vector Machine (SVM)\")\n",
        "sns.heatmap(svm_cm, annot = True, cmap=\"Greens\",cbar=False);\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "plt.title(\"Neural Network\")\n",
        "sns.heatmap(nn_cm, annot = True, cmap=\"Greens\",cbar=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FxvnbKuPrZa6"
      },
      "cell_type": "markdown",
      "source": [
        "## <span id=\"15\"></span> Precision - Recall - F1-Score"
      ]
    },
    {
      "metadata": {
        "id": "noOVoubVrZa6"
      },
      "cell_type": "markdown",
      "source": [
        "Last but not least, I examined *Precision*, *Recall* and *F1-score*. They are defined as\n",
        "\n",
        "$$\\textbf{Precision} = \\dfrac{TP}{TP + FP} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\textbf{Recall} = \\dfrac{TP}{TP + FN}$$\n",
        "\n",
        "\n",
        "$$\\textbf{}$$\n",
        "\n",
        "\n",
        "$$\\textbf{F}_{1} = 2 \\; \\dfrac{Precision \\; \\times \\;Recall}{Precision + Recall} = \\dfrac{2 \\; TP}{2 \\; TP + FN + FP}$$\n",
        "\n",
        "We have already examined ROC curves and confusion matrices but this is not enough for a final decision. In our case precision might be the best choice to evaluate our models beacause we want to determine negative comments with less mistakes (To compare precisions, we must look at the precision values for *True*.). However, if we predict positive comments false, it will not have a negative impact. Of course for some other purposes other evaluation metrics may be useful too."
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "0NHtEX_3rZa7"
      },
      "cell_type": "code",
      "source": [
        "print(\"Logistic Regression\")\n",
        "print(mt.classification_report(y_test, lr.predict(X_test)))\n",
        "print(\"\\n Naive Bayes\")\n",
        "print(mt.classification_report(y_test, nb.predict(X_test)))\n",
        "print(\"\\n Support Vector Machine (SVM)\")\n",
        "print(mt.classification_report(y_test, svm.predict(X_test)))\n",
        "print(\"\\n Neural Network\")\n",
        "print(mt.classification_report(y_test, nn.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ef38d9393b73d3a1bf6f14b4094f54c634aaabb3",
        "id": "oefBdlfXrZa7"
      },
      "cell_type": "markdown",
      "source": [
        "# <span id=\"16\"></span> Conclusion\n",
        "#### [Return Contents](#0)\n",
        "<hr/>"
      ]
    },
    {
      "metadata": {
        "_uuid": "527bf5e24b62b2993574a1ab64e2fa34c08bbd49",
        "id": "0Tu41nkrrZa7"
      },
      "cell_type": "markdown",
      "source": [
        "When we look at the results of the all evaluation metrics in the *evaluating models section*, **Naive Bayes** and **Logistic Regression** gives the best results for our analysis. Thus, both of them are very effective at predicting sentiment. On the other hand, it seems that **Naive Bayes** takes less time and when we have a bigger dataset, this difference might increase and be an important advantage .\n",
        "\n",
        "<b><font color=\"green\">Thank you for reading my kernel </font></b> **and If you liked this kernel, please** <b><font color=\"red\">do not forget to <b></font><font color=\"green\">UPVOTE </font></b> ðŸ™‚"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}